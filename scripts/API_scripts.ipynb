{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b17b3-403f-4c31-833a-e03a2cfda310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#object_names\n",
    "\n",
    "label_file = 'object_names.xlsx'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# read by default 1st sheet of an excel file\n",
    "dataframe1 = pd.read_excel(label_file)\n",
    "labels = np.array(dataframe1).squeeze()\n",
    " \n",
    "print(len(labels))\n",
    "\n",
    "#for label_pair in labels:\n",
    "#    print(label_pair[0],label_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a8846-2e06-4a0f-b019-1060ff682ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_images = \"stimuli_exp/\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "filenames = sorted([f for f in listdir(path_images) if isfile(join(path_images, f))])[1:]\n",
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f40e7-e40e-4fc6-b7fd-9aff4a03ccbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79d4fe-28ac-4ec2-9555-115eaeacd110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0d401-5105-4adc-b618-1a25aec4694b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48433ab4-a000-4629-b42b-441d579ef396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI models:\n",
    "\n",
    "%env OPENAI_API_KEY= # your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be46dc-032b-4dab-9560-c4c5172ea0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11632323-ea26-4609-b803-878cd5a859ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903e5fb-86b1-4a2a-abd0-5eb251fc2bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"your key\"\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "previous_label = \"nothing\"\n",
    "label_before_that = \"nothing\"\n",
    "i = 0\n",
    "while i < len(labels):\n",
    "    \n",
    "    label1 = labels[i][0]\n",
    "\n",
    "        \n",
    "    if label1 != previous_label:\n",
    "        condition = 0\n",
    "    elif label1 != label_before_that:\n",
    "        condition = 1\n",
    "    else:\n",
    "        condition = 2\n",
    "          \n",
    "    \n",
    "    \n",
    "    label2 = labels[i][1]\n",
    "    image_path =filenames[i]\n",
    "    # Path to your image\n",
    "    full_image_path = path_images + image_path\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(full_image_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    What's in this image?\n",
    "    A. {label1}\n",
    "    B. {label2}\n",
    "\n",
    "    Choose either A or B and answer in one or two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "              },\n",
    "              {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                  \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0  # temperature controls the randomness of the output\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    \n",
    "    if 'error' in response.json():\n",
    "        print(\"error. pausing for 60sec\")\n",
    "        print(\"\")\n",
    "        time.sleep(60)\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        print(\"Id: \" + str(i) + \". Condition: \" + str(condition)) \n",
    "        print(prompt)\n",
    "        image = Image.open(full_image_path)\n",
    "        display(image.resize((100,100)))      \n",
    "        print()\n",
    "        print(\"GPT-4o: \" + response.json()['choices'][0]['message']['content'])\n",
    "        label_before_that = previous_label\n",
    "        previous_label = label1\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7dbf0-3763-4638-9ca2-b76a052b3dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b54ad4-3792-4f80-9760-2f55509f46a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPT-4-vision-preview\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# OpenAI API Key\n",
    "api_key = \"your key\"\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "previous_label = \"nothing\"\n",
    "label_before_that = \"nothing\"\n",
    "i = 0\n",
    "while i < len(labels):\n",
    "    \n",
    "    label1 = labels[i][0]\n",
    "\n",
    "        \n",
    "    if label1 != previous_label:\n",
    "        condition = 0\n",
    "    elif label1 != label_before_that:\n",
    "        condition = 1\n",
    "    else:\n",
    "        condition = 2\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    label2 = labels[i][1]\n",
    "    image_path =filenames[i]\n",
    "    # Path to your image\n",
    "    full_image_path = path_images + image_path\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(full_image_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    What's in this image?\n",
    "    A. {label1}\n",
    "    B. {label2}\n",
    "\n",
    "    Choose either A or B and answer in one or two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "              },\n",
    "              {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                  \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ],\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0  # temperature controls the randomness of the output\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    \n",
    "    if 'error' in response.json():\n",
    "        print(\"error. pausing for 60sec\")\n",
    "        print(\"\")\n",
    "        time.sleep(60)\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        print(\"Id: \" + str(i) + \". Condition: \" + str(condition)) \n",
    "        print(prompt)\n",
    "        image = Image.open(full_image_path)\n",
    "        display(image.resize((100,100)))      \n",
    "        print()\n",
    "        print(\"GPT-4-vision-preview: \" + response.json()['choices'][0]['message']['content'])\n",
    "        label_before_that = previous_label\n",
    "        previous_label = label1\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ff4a1-ce0b-4448-bccd-a3d63f39aa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1998a-4fb4-405c-a38f-bb4584d233bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c19c38-bb44-44f0-908c-78ab59261e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemini models\n",
    "\n",
    "GOOGLE_API_KEY = # your key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240038a4-d3b9-461b-a95c-d0478e2fbfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e37ca3-8236-4d8d-9e88-ff20ec97cf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2e07f-626b-4495-984b-36a82fd0de44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9325e8-89ec-4d89-a7d6-b9f58ff11e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639898df-d882-43fd-8c7d-fba89b559f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemini Flash 1.5\n",
    "\n",
    "from PIL import Image\n",
    "    \n",
    "previous_label = \"nothing\"\n",
    "label_before_that = \"nothing\"    \n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label1 = labels[i][0]\n",
    "    \n",
    "    if label1 != previous_label:\n",
    "        condition = 0\n",
    "    elif label1 != label_before_that:\n",
    "        condition = 1\n",
    "    else:\n",
    "        condition = 2\n",
    "        \n",
    "    label2 = labels[i][1]\n",
    "    image_path = filenames[i]\n",
    "    full_image_path = path_images + image_path\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    What's in this image?\n",
    "    A. {label1}\n",
    "    B. {label2}\n",
    "\n",
    "    Choose either A or B and answer in one or two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-1.5-flash', \n",
    "                                 generation_config=genai.GenerationConfig(\n",
    "        temperature=0,\n",
    "    ))\n",
    "    image = Image.open(full_image_path)\n",
    "    \n",
    "    try: \n",
    "        response = model.generate_content([prompt, image])\n",
    "        print()\n",
    "        print(\"Id: \" + str(i) + \". Condition: \" + str(condition)) \n",
    "        print(prompt)\n",
    "        display(image.resize((100, 100)))\n",
    "        print()\n",
    "        print('Gemini 1.5 Flash: {}'.format(response.text))\n",
    "        \n",
    "    except Exception as e:  \n",
    "        print(f\"Error encountered for image {i}: {e}\")\n",
    "  \n",
    "    label_before_that = previous_label\n",
    "    previous_label = label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435c6e3-ac84-4def-89de-174f36408929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b575a8-bb1c-4ba9-a8f1-b75e8d068065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemini 1.5 Pro\n",
    "\n",
    "from PIL import Image\n",
    "    \n",
    "previous_label = \"nothing\"\n",
    "label_before_that = \"nothing\"    \n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label1 = labels[i][0]\n",
    "    \n",
    "    if label1 != previous_label:\n",
    "        condition = 0\n",
    "    elif label1 != label_before_that:\n",
    "        condition = 1\n",
    "    else:\n",
    "        condition = 2\n",
    "        \n",
    "    label2 = labels[i][1]\n",
    "    image_path = filenames[i]\n",
    "    full_image_path = path_images + image_path\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    What's in this image?\n",
    "    A. {label1}\n",
    "    B. {label2}\n",
    "\n",
    "    Choose either A or B and answer in one or two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-1.5-pro', \n",
    "                                 generation_config=genai.GenerationConfig(\n",
    "        temperature=0,\n",
    "    ))\n",
    "    image = Image.open(full_image_path)\n",
    "    \n",
    "    try: \n",
    "        response = model.generate_content([prompt, image])\n",
    "        print()\n",
    "        print(\"Id: \" + str(i) + \". Condition: \" + str(condition)) \n",
    "        print(prompt)\n",
    "        display(image.resize((100, 100)))\n",
    "        print()\n",
    "        print('Gemini 1.5 Pro: {}'.format(response.text))\n",
    "        \n",
    "    except Exception as e:  \n",
    "        print(f\"Error encountered for image {i}: {e}\")\n",
    "  \n",
    "    label_before_that = previous_label\n",
    "    previous_label = label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d7b2c-abf7-4bba-bedc-fb19a49602c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9663b4-328b-44d2-af77-edc4f758feec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b5d98-80c5-4d5e-a2a7-f7c78630d4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Claude models\n",
    "\n",
    "claude_key = # your key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce2c88-0873-4075-9157-37e44a048adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc4a32-652e-4386-8464-83e69ee13d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# claude 3 opus\n",
    "\n",
    "import anthropic\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=claude_key,\n",
    ")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    \n",
    "previous_label = \"nothing\"\n",
    "label_before_that = \"nothing\"\n",
    "i = 0\n",
    "while i < len(labels):\n",
    "    \n",
    "    label1 = labels[i][0]\n",
    "\n",
    "        \n",
    "    if label1 != previous_label:\n",
    "        condition = 0\n",
    "    elif label1 != label_before_that:\n",
    "        condition = 1\n",
    "    else:\n",
    "        condition = 2\n",
    "        \n",
    "    \n",
    "    label2 = labels[i][1]\n",
    "    image_path =filenames[i]\n",
    "    # Path to your image\n",
    "    full_image_path = path_images + image_path\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(full_image_path)\n",
    "    image_media_type = \"image/png\"\n",
    "    \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    What's in this image?\n",
    "    A. {label1}\n",
    "    B. {label2}\n",
    "\n",
    "    Choose either A or B and answer in one or two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image_media_type,\n",
    "                        \"data\": base64_image,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "    \n",
    "    print()\n",
    "    print(\"Id: \" + str(i) + \". Condition: \" + str(condition)) \n",
    "    print(prompt)\n",
    "    image = Image.open(full_image_path)\n",
    "    display(image.resize((100,100)))\n",
    "    print()\n",
    "    print(f\"Claude 3 Opus: {message.content[0].text}\")\n",
    "  \n",
    "    label_before_that = previous_label\n",
    "    previous_label = label1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536bdb5-5b67-4bb9-b47e-3f6a3588218c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c5da3-a308-48e6-acd3-d4ff93d29f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# claude 3.5 sonnet\n",
    "\n",
    "import anthropic\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=claude_key,\n",
    ")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    \n",
    "previous_label = \"nothing\"\n",
    "label_before_that = \"nothing\"\n",
    "i = 0\n",
    "while i < len(labels):\n",
    "    \n",
    "    label1 = labels[i][0]\n",
    "\n",
    "        \n",
    "    if label1 != previous_label:\n",
    "        condition = 0\n",
    "    elif label1 != label_before_that:\n",
    "        condition = 1\n",
    "    else:\n",
    "        condition = 2\n",
    "        \n",
    "    \n",
    "    label2 = labels[i][1]\n",
    "    image_path =filenames[i]\n",
    "    # Path to your image\n",
    "    full_image_path = path_images + image_path\n",
    "\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(full_image_path)\n",
    "    image_media_type = \"image/png\"\n",
    "    \n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    What's in this image?\n",
    "    A. {label1}\n",
    "    B. {label2}\n",
    "\n",
    "    Choose either A or B and answer in one or two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image_media_type,\n",
    "                        \"data\": base64_image,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "    \n",
    "    print()\n",
    "    print(\"Id: \" + str(i) + \". Condition: \" + str(condition)) \n",
    "    print(prompt)\n",
    "    image = Image.open(full_image_path)\n",
    "    display(image.resize((100,100)))\n",
    "    print()\n",
    "    print(f\"Claude 3.5 Sonnet: {message.content[0].text}\")\n",
    "  \n",
    "    label_before_that = previous_label\n",
    "    previous_label = label1\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04524c-39af-4ba9-80fa-2504b92f3164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
